{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fbbb899",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "1. Implement a function that takes 2-, 3-, or 4-grams to generate text, \n",
    "    if an initial N−1 gram is given, by sampling a next token according \n",
    "    to the conditional distribution $P(w_N|w_1, ..., w_{N−1})$.\n",
    "2. If the initial N−1 gram is unknown, there is no distribution to sample\n",
    "    from  and the generation process cannot start. A way out is to “reverse”\n",
    "    the backoff-idea for N-grams: i.e. if $P(w_N|w_1, ..., w_{N−1})$ is\n",
    "    unknown, I can try $P(w_N|w_2, ..., w_{N−1})$, or \n",
    "    $P(w_N|w_3, ..., w_{N−1})$, ... or $P(w_N|w_{N−1})$, or at last $P(w_N)$.\n",
    "\n",
    "    \n",
    "These distributions have already been estimated in subtask 1.\n",
    "Update your code such that it implements this idea!\n",
    "\n",
    "Does it help only for the initial N−1 gram or also later in the generation process?\n",
    "\n",
    "Ans. Backoff also helps during the generation process as well as during initialization.\n",
    "When higher order n-grams are not available, we try lower order n-grams. Whenever a prefix is unknown during the generation process, we try a smaller prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e4fc879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df = pd.read_csv('imdb_reviews.csv')\n",
    "unigram_vectorizer = CountVectorizer(ngram_range=(1,1))\n",
    "bigram_vectorizer =  CountVectorizer(ngram_range=(2,2))\n",
    "trigram_vectorizer = CountVectorizer(ngram_range=(3,3))\n",
    "fourgram_vectorizer = CountVectorizer(ngram_range=(4,4))\n",
    "\n",
    "X_unigram = unigram_vectorizer.fit_transform(df['review'])\n",
    "X_bigram  = bigram_vectorizer.fit_transform(df['review'])\n",
    "X_trigram = trigram_vectorizer.fit_transform(df['review'])\n",
    "X_fourgram = fourgram_vectorizer.fit_transform(df['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3316ef34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# maps the value of n -> frequency map.\n",
    "ngram_freq_maps = {}\n",
    "\n",
    "ngram_freq_maps[1] = dict(zip(\n",
    "    unigram_vectorizer.get_feature_names_out(),\n",
    "    X_unigram.sum(axis=0).A1\n",
    "))\n",
    "\n",
    "ngram_freq_maps[2] = dict(zip(\n",
    "    bigram_vectorizer.get_feature_names_out(),\n",
    "    X_bigram.sum(axis=0).A1\n",
    "))\n",
    "\n",
    "ngram_freq_maps[3] = dict(zip(\n",
    "    trigram_vectorizer.get_feature_names_out(),\n",
    "    X_trigram.sum(axis=0).A1\n",
    "))\n",
    "\n",
    "ngram_freq_maps[4] = dict(zip(\n",
    "    fourgram_vectorizer.get_feature_names_out(),\n",
    "    X_fourgram.sum(axis=0).A1\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1499420d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute conditional prob dictionaries from freq maps\n",
    "ngram_prob_maps = {}\n",
    "\n",
    "# for 2,3,4-grams\n",
    "for n in range(2, 5):\n",
    "    prob_dict = {}\n",
    "\n",
    "    for ngram, count in ngram_freq_maps[n].items():\n",
    "        # suppose the tokens are `cat is cute`\n",
    "        tokens = ngram.split()\n",
    "\n",
    "        # context is `cat is` and next_word is `cute`\n",
    "        context = \" \".join(tokens[:-1])\n",
    "        next_word = tokens[-1]\n",
    "\n",
    "        # Count(`cat is`)\n",
    "        context_count = ngram_freq_maps[n-1].get(context, 0)\n",
    "\n",
    "        if context_count > 0:\n",
    "            # P(`cute` | `cat is`) = Count(`cat is cute`) / Count(`cat is`)\n",
    "            prob = count / context_count\n",
    "\n",
    "            if context not in prob_dict:\n",
    "                prob_dict[context] = {}\n",
    "\n",
    "            # ngram_prob_maps[3][`cat is`] = { `cute`: number_found_above }\n",
    "            prob_dict[context][next_word] = prob\n",
    "\n",
    "    ngram_prob_maps[n] = prob_dict\n",
    "\n",
    "# For unigrams (fallback), normalize to probs\n",
    "total_unigrams = sum(ngram_freq_maps[1].values())\n",
    "unigram_probs = {word: freq / total_unigrams for word, freq in ngram_freq_maps[1].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d15238b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def generate_text_bigram(initial_prefix: str, max_length: int=50):\n",
    "    \"\"\"\n",
    "        Generate text using bigram model. Whenever, a bigram is not available\n",
    "        the generation process halts.\n",
    "    \"\"\"\n",
    "    generated = initial_prefix.split()\n",
    "    current_prefix = generated[-1]  # last word\n",
    "\n",
    "    for _ in range(max_length - len(generated)):\n",
    "        next_word = None\n",
    "\n",
    "        if current_prefix in ngram_prob_maps[2]:\n",
    "            probs = ngram_prob_maps[2][current_prefix]\n",
    "            next_word = random.choices(list(probs.keys()), weights=list(probs.values()))[0]\n",
    "        else:\n",
    "            break\n",
    "\n",
    "        generated.append(next_word)\n",
    "        current_prefix = next_word\n",
    "\n",
    "    return \" \".join(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcb0ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_trigram(initial_prefix: str, max_length: int=50):\n",
    "    \"\"\"\n",
    "        Generate text using trigram model.\n",
    "    \"\"\"\n",
    "    generated = initial_prefix.split()\n",
    "    current_prefix = \" \".join( generated[ len(generated)-2: ] ) # last 2 words\n",
    "\n",
    "    for _ in range(max_length - len(generated)):\n",
    "        next_word = None\n",
    "\n",
    "        if current_prefix in ngram_prob_maps[3]:\n",
    "            probs = ngram_prob_maps[3][current_prefix]\n",
    "            next_word = random.choices(list(probs.keys()), weights=list(probs.values()))[0]\n",
    "        else:\n",
    "            break\n",
    "\n",
    "        generated.append(next_word)\n",
    "        current_prefix = \" \".join(generated[-2:]) # last 2 words\n",
    "\n",
    "    return \" \".join(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dd7828a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_fourgram(initial_prefix: str, max_length: int=50):\n",
    "    \"\"\"\n",
    "        Generate text using fourgram model.\n",
    "    \"\"\"\n",
    "    generated = initial_prefix.split()\n",
    "    current_prefix = \" \".join( generated[ len(generated)-3: ] ) # last 3 words\n",
    "\n",
    "    for _ in range(max_length - len(generated)):\n",
    "        next_word = None\n",
    "\n",
    "        if current_prefix in ngram_prob_maps[4]:\n",
    "            probs = ngram_prob_maps[4][current_prefix]\n",
    "            next_word = random.choices(list(probs.keys()), weights=list(probs.values()))[0]\n",
    "        else:\n",
    "            break\n",
    "\n",
    "        generated.append(next_word)\n",
    "        current_prefix = \" \".join(generated[-3:]) # last 3 words\n",
    "\n",
    "    return \" \".join(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "20242ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " **********  Text generated by Bigram Model  ********** \n",
      "\n",
      "\n",
      "For prefix: the,\n",
      "Text Generated by bigram model: the point of the same reason why he just let go to the bellicose sergeant forced at his own didn have ever there is no grand canyon because it isn long distance though the two become the confession br enter into an adult cinema br br br will without being\n",
      "\n",
      "\n",
      "For prefix: this,\n",
      "Text Generated by bigram model: this movie goers who controlled by cheh was excellent widmark is very enjoyable and blond american history because he is especially for the acting but full nudity and gretchen mol nude swimming in the town of disaster has funny movie timon timon and you would have borrowed from the rope\n",
      "\n",
      "\n",
      " **********  Text generated by Trigram Model  ********** \n",
      "\n",
      "\n",
      "For prefix: the movie,\n",
      "Text Generated by trigram model: the movie that exemplifies hope love and happiness freud might have thought that an artist profound vision his art is namely the french first dubbed noir was without any intention to resolve is jake speed they are and it primes her vulnerability when patient billy hahn steven goldstein confides to\n",
      "\n",
      "\n",
      "For prefix: this movie,\n",
      "Text Generated by trigram model: this movie as child probably because was having and grandpa living at the mafia frank anselmo corrupt judicial officials like judge stern who were put off buy the movie used and manipulated father yet also leaves room for anything found it both ways too br br amazing stories have ever\n",
      "\n",
      "\n",
      " **********  Text generated by 4-Gram Model  ********** \n",
      "\n",
      "\n",
      "For prefix: this movie is,\n",
      "Text Generated by fourgram model: this movie is one of my favourites because it was made for television rather than the elegance of versailles or the massive megalomania of feudal china tang dynasty whose self serving seeking of power brings ruin upon many others and forces an extremely christian interpretation on film which is as\n",
      "\n",
      "\n",
      "For prefix: the movie is,\n",
      "Text Generated by fourgram model: the movie is fantasy the aborigines are invested with the kind of movie to show that there are no jokes whatsoever so if you like watching movies to get all your nerves excited through on screen br br dudley moore makes character that could have done without many of the\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_prefixes_bigram = [ \"the\", \"this\" ]\n",
    "test_prefixes_trigram = [ \"the movie\", \"this movie\" ]\n",
    "test_prefix_fourgram = [ \"this movie is\", \"the movie is\" ]\n",
    "\n",
    "print( \"\\n\", \"*\" * 10, \" Text generated by Bigram Model \", \"*\" * 10, \"\\n\")\n",
    "\n",
    "for p in test_prefixes_bigram:\n",
    "    print(f'\\nFor prefix: {p},\\nText Generated by bigram model: {generate_text_bigram(p)}\\n')\n",
    "\n",
    "print( \"\\n\", \"*\" * 10, \" Text generated by Trigram Model \", \"*\" * 10, \"\\n\")\n",
    "\n",
    "for p in test_prefixes_trigram:\n",
    "    print(f'\\nFor prefix: {p},\\nText Generated by trigram model: {generate_text_trigram(p)}\\n')\n",
    "\n",
    "print( \"\\n\", \"*\" * 10, \" Text generated by 4-Gram Model \", \"*\" * 10, \"\\n\")\n",
    "\n",
    "for p in test_prefix_fourgram:\n",
    "    print(f'\\nFor prefix: {p},\\nText Generated by fourgram model: {generate_text_fourgram(p)}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1695f2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_ngrams_model(n: int, initial_prefix: str, max_length: int=50):\n",
    "    \"\"\"\n",
    "        Generate text using n-grams model. Assume that initial n-1 words are\n",
    "        given as prefix. Uses backoff strategy to shorter n-grams if larger\n",
    "        n-grams are not found.\n",
    "    \"\"\"\n",
    "    generated = initial_prefix.split()\n",
    "    current_prefix = initial_prefix\n",
    "\n",
    "    for _ in range(max_length - len(generated)):\n",
    "        next_word = None\n",
    "\n",
    "        prefix_tokens = current_prefix.split()\n",
    "\n",
    "        for backoff_level in range(len(prefix_tokens) + 1):  # 0: full, 1: remove first\n",
    "\n",
    "            # suppose we have current prefix \"the movie\"\n",
    "            test_prefix = \" \".join(prefix_tokens[backoff_level:])\n",
    "\n",
    "            # check if \"the movie\" is in ngrams_prob_maps[3]\n",
    "            # if no -> increase backoff, make the prefix \"movie\" next time\n",
    "\n",
    "            if not test_prefix:\n",
    "                # the test prefix is empty, maybe because we do not have enough tokens left after backoff\n",
    "                # in this case we randomly select from unigram probabilities\n",
    "                next_word = random.choices(list(unigram_probs.keys()), weights=list(unigram_probs.values()))[0]\n",
    "                break\n",
    "\n",
    "            elif test_prefix in ngram_prob_maps[n]:\n",
    "                # \"the movie\" is found in ngram_prob_maps[3]\n",
    "                probs = ngram_prob_maps[n][test_prefix]\n",
    "                # we will have probs like: {  \"was\": 0.6, \"is\": 0.4 }\n",
    "                next_word = random.choices(list(probs.keys()), weights=list(probs.values()))[0]\n",
    "                # we choose one of these words\n",
    "                break\n",
    "\n",
    "        if not next_word:\n",
    "            break  # No options, halt generation\n",
    "\n",
    "        generated.append(next_word)\n",
    "\n",
    "        # Update prefix to last N-1 words\n",
    "        current_prefix = \" \".join(generated[-(n-1):])\n",
    "\n",
    "    return \" \".join(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6bb3dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating text with 2-grams model ---\n",
      "\n",
      "Prefix: the movie was\n",
      "Generated: the movie was spitfire of the liquefied colors aid in this was ignored by alfio contini find out of main character development\n",
      "\n",
      "Prefix: i really enjoyed\n",
      "Generated: i really enjoyed mostly forgotten today are all male to me lot from quick lover júlio ismael de palma direction is master\n",
      "\n",
      "Prefix: the plot of\n",
      "Generated: the plot of africa and you see this movie in mere sport and involves lust temptation of them thinking fondly through the\n",
      "\n",
      "Prefix: the acting was\n",
      "Generated: the acting was great national spirit of 10 10 br br episode is the prez prado and closing credits br this matter\n",
      "\n",
      "Prefix: i would recommend\n",
      "Generated: i would recommend it came on youtube but dialogue focusing on tap ii movies my life is perfectly balanced writing this may\n",
      "\n",
      "\n",
      "--- Generating text with 3-grams model ---\n",
      "\n",
      "Prefix: the movie was\n",
      "Generated: the movie was just too amazing to see it twice about making things drag on and on about how the film\n",
      "\n",
      "Prefix: i really enjoyed\n",
      "Generated: i really enjoyed this film is lost anyway highly recommended particularly if you love no matter this is thriller that clearly\n",
      "\n",
      "Prefix: the plot of\n",
      "Generated: the plot of the book or movie which have not watched it alone what brownstone apartment is the penchant of french\n",
      "\n",
      "Prefix: the acting was\n",
      "Generated: the acting was convincing am biker they reminded me quite bit more disenchantment for what it takes an interest when nothing\n",
      "\n",
      "Prefix: i would recommend\n",
      "Generated: i would recommend it as simple brute but his most humane and one quickly learns early that gardenia odd behavior the\n",
      "\n",
      "\n",
      "--- Generating text with 4-grams model ---\n",
      "\n",
      "Prefix: the movie was\n",
      "Generated: the movie was as predictable as the rest of the cast though several faces will be familiar the entire cast\n",
      "\n",
      "Prefix: i really enjoyed\n",
      "Generated: i really enjoyed as that portrait only fact issue mamooth am doubt congratulations in is has terrorists nations system just\n",
      "\n",
      "Prefix: the plot of\n",
      "Generated: the plot of the first novelized feminists she finally accepted love on her own with all these ingredients at hand\n",
      "\n",
      "Prefix: the acting was\n",
      "Generated: the acting was fresh and the dancing to this day br br there are so many conflicting stories as everyone\n",
      "\n",
      "Prefix: i would recommend\n",
      "Generated: i would recommend the tv adaptation is visually striking with some lovely photography and very strong climactic battle scene charlton\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_prefixes = [\n",
    "    \"the movie was\",\n",
    "    \"i really enjoyed\",\n",
    "    \"the plot of\",\n",
    "    \"the acting was\",\n",
    "    \"i would recommend\",\n",
    "]\n",
    "\n",
    "for n in range(2,5):\n",
    "    print(f\"\\n--- Generating text with {n}-grams model ---\\n\")\n",
    "    for prefix in test_prefixes:\n",
    "        generated_text = generate_text_ngrams_model(n, prefix, max_length=20)\n",
    "        print(f\"Prefix: {prefix}\\nGenerated: {generated_text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4bd5cb",
   "metadata": {},
   "source": [
    "## Sampling initial prefixes for 3-grams model\n",
    "\n",
    "We also need to generate 5000 samples for naive bayes classifier. We generate these reviews\n",
    "using 3-gram model. But for that, we need 2 words for initial generation for each review. \n",
    "Hence, we need 5000 initial prefixes of size 2. For example: \"the movie\", \"it is\", \"i like\" etc.\n",
    "\n",
    "We simply sample initial prefixes from our `imdb_reviews.csv` dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62ab2aff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>initial_prefix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This is a great movie for the true romantics a...</td>\n",
       "      <td>This is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I saw this film when I was a young child on te...</td>\n",
       "      <td>I saw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I consider myself a great admirer of David Lyn...</td>\n",
       "      <td>I consider</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cat Soup at first seems to be a very random an...</td>\n",
       "      <td>Cat Soup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Back in 1994, I had a really lengthy vacation ...</td>\n",
       "      <td>Back in</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review initial_prefix\n",
       "0  This is a great movie for the true romantics a...        This is\n",
       "1  I saw this film when I was a young child on te...          I saw\n",
       "2  I consider myself a great admirer of David Lyn...     I consider\n",
       "3  Cat Soup at first seems to be a very random an...       Cat Soup\n",
       "4  Back in 1994, I had a really lengthy vacation ...        Back in"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# keep the initial 2 words for each review\n",
    "df = pd.read_csv('imdb_reviews.csv')\n",
    "df['initial_prefix'] = df['review'].apply(lambda x: ' '.join(x.split()[:2]))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09d3e008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('this is', np.int64(706)),\n",
       "  ('this movie', np.int64(418)),\n",
       "  ('this film', np.int64(195)),\n",
       "  ('this was', np.int64(117)),\n",
       "  ('if you', np.int64(115)),\n",
       "  ('one of', np.int64(92)),\n",
       "  ('the movie', np.int64(59)),\n",
       "  ('br br', np.int64(56)),\n",
       "  ('in the', np.int64(50)),\n",
       "  ('the first', np.int64(45))],\n",
       " 4711)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute bigram frequencies, and choose 500 prefiex based on weights = frequencies\n",
    "\n",
    "bigram_vectorizer_initial_prefix = CountVectorizer(ngram_range=(2,2))\n",
    "X_bigram_initial_prefix  = bigram_vectorizer_initial_prefix.fit_transform(df['initial_prefix'])\n",
    "\n",
    "freq_initial_prefix = dict(zip(\n",
    "    bigram_vectorizer_initial_prefix.get_feature_names_out(),\n",
    "    X_bigram_initial_prefix.sum(axis=0).A1\n",
    "))\n",
    "\n",
    "## check top 10 most frequent initial prefixes\n",
    "sorted(freq_initial_prefix.items(), key=lambda x: x[1], reverse=True)[:10], len(freq_initial_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9bd2e4",
   "metadata": {},
   "source": [
    ">##### We only have `4711` unique initial prefixes in this dataset. But we will still end up generating `5000` reviews, since we can generate multiple unique reviews from same initial biagrams. Also, lets only keep top 2000 initial prefixes, because there is some noise in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d38bd2b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>initial_prefix</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this is</td>\n",
       "      <td>706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this movie</td>\n",
       "      <td>418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this film</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this was</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>if you</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  initial_prefix  frequency\n",
       "0        this is        706\n",
       "1     this movie        418\n",
       "2      this film        195\n",
       "3       this was        117\n",
       "4         if you        115"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_2000_initial_prefixes = sorted(freq_initial_prefix.items(), key=lambda x: x[1], reverse=True)[:2000]\n",
    "\n",
    "initial_prefixes = pd.DataFrame(\n",
    "    {\n",
    "        \"initial_prefix\": [x[0] for x in top_2000_initial_prefixes ],\n",
    "        \"frequency\":      [x[1] for x in top_2000_initial_prefixes]\n",
    "    }\n",
    ")\n",
    "initial_prefixes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "663d4d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_prefixes.to_csv('initial_prefixes.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b868d6",
   "metadata": {},
   "source": [
    "## Generating 5000 artificial reviews for Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aba7c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "artificial_reviews = {\n",
    "    \"artificial_reviews\": []\n",
    "}\n",
    "\n",
    "# note that the probability fo selecting an initial prefix depends on its\n",
    "# frequency in original dataset\n",
    "\n",
    "for ip in random.choices(initial_prefixes[\"initial_prefix\"], initial_prefixes[\"frequency\"], k=5000 ):\n",
    "    # review of random length in range [30,50]\n",
    "    review = generate_text_ngrams_model(3, ip, random.randint(30,50) )\n",
    "    artificial_reviews[\"artificial_reviews\"].append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc65cd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the artificial reviews\n",
    "pd.DataFrame(artificial_reviews).to_csv(\"artificial_reviews.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369cceaa",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
