{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92a3d215",
   "metadata": {},
   "source": [
    "## Language Models\n",
    "\n",
    "Suppose we have ($n-1$) words, and we want to predict the $n^{th}$ word.\n",
    "\n",
    "The probability for a particular $n^{th}$ word i.e. $w_n$ is given by:\n",
    "\n",
    "$$\n",
    "P(w_n | w_1, w_2, \\ldots, w_{n-1})\n",
    "$$\n",
    "\n",
    "That is, probability of word $w_n$, given we already have words: $w_1, w_2, \\ldots, w_{n-1}$\n",
    "\n",
    "This is same as conditional probability of $w_n$ given $w_1, w_2, \\ldots, w_{n-1}$ already occur.\n",
    "\n",
    "A model, that computes this probability is called a **language model**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d5cb89",
   "metadata": {},
   "source": [
    "## Maximum Likelyhood Estimation (MLE)\n",
    "\n",
    "How can we count these probabilities? A simple approach is called maximum \n",
    "likelyhood estimate. \n",
    "\n",
    "### A Simple Example: Word Prediction\n",
    "\n",
    "Your goal is to predict the next word. You have this tiny corpus (collection of text):\n",
    "\n",
    "> \"the cat sat on the mat. the cat slept.\"\n",
    "\n",
    "Let's estimate the probability `P(mat | on the)`. <br>\n",
    "That is, given the words \"on the\", what's the chance the next word is \"mat\"?\n",
    "\n",
    "1.  **Count the sequence**: How many times do we see \"**on the**\" in the corpus?\n",
    "    *   The sequences are: `[the, cat]`, `[cat, sat]`, `[sat, on]`, `[on, the]`, <br>\n",
    "            `[the, mat]`, `[mat, the]`, `[the, cat]`, `[cat, slept]`.\n",
    "    *   `\"on the\"` appears **once**.\n",
    "\n",
    "2.  **Count the specific outcome**: How many times does \"**on the**\" is followed by \"**mat**\"?\n",
    "    *   Looking at the sequence `[on, the, mat]`, it happens **once**.\n",
    "\n",
    "3.  **Compute the Relative Frequency (The MLE)**:\n",
    "\n",
    "    $$\n",
    "    P_{MLE}(\\text{``mat''}|\\text{``on the''}) = \n",
    "    \\frac{ Count(\\text{``on the mat''}) } { Count(\\text{``on the''}) } =\n",
    "    \\frac{1}{1} = 1\n",
    "    $$\n",
    "\n",
    "According to MLE, if you see \"on the\", the next word will *always* be \"mat\" because that's all we've observed.\n",
    "\n",
    "<br>\n",
    "\n",
    "Hence, the probability $P(w_n | w_1, \\ldots, w_{n-1}, w_{n-2})$ is given by:\n",
    "\n",
    "$$\n",
    "P_{MLE}(w_n | w_{1}, w_2 \\ldots, w_{n-1}) = \n",
    "\\frac{Count(w_1, w_2, \\ldots, w_{n-1}, w_{n})}{Count(w_1, w_2, \\ldots, w_{n-1})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1c2914",
   "metadata": {},
   "source": [
    "## Markov assumption\n",
    "\n",
    "In general, we want `n` to be as large as possible. However, in practice, most events of encountering\n",
    "sequences of words of length greater than 3 hardly ever occur in our corpora!\n",
    "\n",
    "The markov assumption states that:\n",
    "\n",
    "$$\n",
    "\\text{``Only the prior local context – the last few words –\n",
    "affects the next word''}\n",
    "$$\n",
    "\n",
    "\n",
    "Suppose, we assume that for a given word, only the previous $N$ (capital `N`) are required.\n",
    "Then the probability of the $n_{th}$ word is given by:\n",
    "\n",
    "$$\n",
    "P(w_n | w_1, w_2, \\ldots, w_{n-1}) \\approx\n",
    "P(w_n | w_{(n-N+1)}, \\ldots, w_{n-2}, w_{n-1})\n",
    "$$\n",
    "\n",
    "This is known as **N grams model**.\n",
    "\n",
    "| N grams model | value of N    |\n",
    "|---            |---            |\n",
    "| unigram       | 1             |\n",
    "| biagram       | 2             |\n",
    "| trigram       | 3             |\n",
    "\n",
    "\n",
    "- Unigram: P(dog)\n",
    "- Bigram: P(dog|big)\n",
    "- Trigram: P(dog|the,big)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ecd042",
   "metadata": {},
   "source": [
    "## Problems and remedies in N grams\n",
    "\n",
    "We might have 0 probabilities for some sequences, which makes it impossible for the \n",
    "model to output unseen sequences, and generalize\n",
    "\n",
    "To solve this issue, we use `smoothing` to assign non-zero probabilities to zero-probability\n",
    "n-grams.\n",
    "\n",
    "Smoothing can be easily implemented through `Laplace's law` which basically adds 1 to all frequencies.\n",
    "\n",
    "Also, we can try lower order n-grams when higher order n-grams are not available. This idea\n",
    "is called `backoff`. For example,\n",
    "\n",
    "$$\n",
    "P(\\text{``the cat is cute''}|\\text{``the cat is''}) =\n",
    "\\frac{ Count(\\text{``the cat is cute''}) } { Count(\\text{``the cat is''}) }\n",
    "$$\n",
    "\n",
    "But what happens when we do not have any `the cat is` in our dataset i.e. Count(\"the cat is\") = 0.\n",
    "In this case, we cannot calculate the 4 grams. And we fall back to 3 grams.\n",
    "\n",
    "$$\n",
    "P(\\text{``cat is cute''}|\\text{``cat is''}) =\n",
    "\\frac{ Count(\\text{``cat is cute''}) } { Count(\\text{``cat is''}) }\n",
    "$$\n",
    "\n",
    "If even 3 grams is not available, then we can try 2 grams and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81459494",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37aba738",
   "metadata": {},
   "source": [
    "## Text Classification\n",
    "\n",
    "In a classification task, we are provided with a document $ d $ \n",
    "and a set of possible classes $C = \\{c_1, c_2, \\ldots \\}$ \n",
    "we are suppose to output a class $ c \\in C $ that applies to $d$.\n",
    "\n",
    "We can use supervised machine learning, to train a model, for this\n",
    "task on a dataset.\n",
    "\n",
    "In this case, we have:\n",
    "\n",
    "\\begin{align}\n",
    "& \\text{a document} \\; d \\\\\n",
    "& \\text{a fixed set of classes} \\quad C = \\{ c_1, c_2, \\ldots \\} \\\\\n",
    "& \\text{a training set of hand labelled documents} \\quad \n",
    "    (d_1, c_2), (d_2, c_2), \\ldots \\\\\n",
    "\\end{align}\n",
    "\n",
    "Output:\n",
    "\n",
    "$$\n",
    "\\text{a learned classifier } \\quad \\gamma: d \\rightarrow c\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9fad41",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier\n",
    "\n",
    "A naive bayes classifier is a supervised machine learning algorithm\n",
    "for classification tasks based on bayes rule.\n",
    "\n",
    "It uses bag of words representation, and assumes that the ordering of \n",
    "words, or their position does not matter.\n",
    "\n",
    "For a document $d$ and a class $c$.\n",
    "\n",
    "$$\n",
    "P(c \\mid d) = \\frac{P(d \\mid c) \\times P(c)} {P(d)}\n",
    "$$\n",
    "\n",
    "The correct class is given by:\n",
    "\n",
    "\\begin{equation*}\n",
    "    c_{MAP} = \\underset{c \\in C}{\\operatorname{argmax}} P(c \\mid d) \\\\\n",
    "            =  \\underset{ c \\in C } { \\operatorname{argmax} } \n",
    "                \\frac{P(d \\mid c) \\times P(c)} {P(d)}\n",
    "\\end{equation*}\n",
    "\n",
    "We can drop the $P(d)$ because the document is fixed to a single denominator, \n",
    "and it makes no difference of our MAP(maximum a posteriori) calculation. \n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "    c_{MAP} = \\underset{c \\in C} { \\text{argmax} } P( d \\mid c ) \\times P(c)\n",
    "\\end{equation}\n",
    "\n",
    "If we represent the document as $n$ features, then:\n",
    "\n",
    "\\begin{equation}\n",
    "    c_{MAP} = \\underset{c \\in C} { \\text{argmax} } P( {x_1, x_2, \\ldots x_n} \\mid c ) \\times P(c)\n",
    "\\end{equation}\n",
    "\n",
    "Naive bayes also assumes conditional independence between input features. Therefore,\n",
    "\n",
    "\\begin{align*}\n",
    "    c_{MAP} &= \\underset{c \\in C} { \\text{argmax} } \\quad P( x_1 \\mid c ) \\times P( x_2 \\mid c ) \n",
    "                                                                \\ldots \\times P( x_n \\mid c ) \\times P(c_j) \\\\\n",
    "    c_{MAP} &= \\underset{c \\in C} { \\text{argmax} } \\quad P(c_j) \\; \\prod P( x_i \\mid c )\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "For a simple model, lets suppose, we use words as features. Then the input features are given by: \n",
    "$(w_1, w_2, \\ldots w_n)$. SUppose, we want to predict a specific class $c_j$ Then:\n",
    "\n",
    "\\begin{align*}\n",
    "    c_{MAP} &= { \\text{argmax} } \\quad P(c_j) \\prod P( w_i \\mid c_j )\n",
    "\\end{align*}\n",
    "\n",
    "We can calculate $P(w_i \\mid c_j)$ using maximum likelyhood estimation as follows:\n",
    "\n",
    "$$\n",
    "    \\hat{P}(w_i \\mid c_j) = \\frac{ Count(w_i, c_j) } { \\underset{{w \\in V }} {\\sum} Count(w, c_j) }\n",
    "$$\n",
    "\n",
    "i.e. first we combine all the documents of class $c_j$ to form collection of words $V$. Then we\n",
    "calculate the frequency of word $w$ in $V$ i.e. all words in $c_j$.\n",
    "\n",
    "Secondly, we also need $P(c_j)$. This can be calculated simply by:\n",
    "\n",
    "$$\n",
    "    P(c_j) = \\frac{ \\text{number of documents of class } c_j } { \\text{total number of documents} }\n",
    "$$\n",
    "\n",
    "These basic ideas form the backbone of Multinomial Naive Bayes Model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbac6d34",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
