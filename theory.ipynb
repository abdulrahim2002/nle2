{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92a3d215",
   "metadata": {},
   "source": [
    "## Language Models\n",
    "\n",
    "Suppose we have ($n-1$) words, and we want to predict the $n^{th}$ word.\n",
    "\n",
    "The probability for a particular $n^{th}$ word i.e. $w_n$ is given by:\n",
    "\n",
    "$$\n",
    "P(w_n | w_1, w_2, \\ldots, w_{n-1})\n",
    "$$\n",
    "\n",
    "That is, probability of word $w_n$, given we already have words: $w_1, w_2, \\ldots, w_{n-1}$\n",
    "\n",
    "This is same as conditional probability of $w_n$ given $w_1, w_2, \\ldots, w_{n-1}$ already occur.\n",
    "\n",
    "A model, that computes this probability is called a **language model**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d5cb89",
   "metadata": {},
   "source": [
    "## Maximum Likelyhood Estimation (MLE)\n",
    "\n",
    "How can we count these probabilities? A simple approach is called maximum \n",
    "likelyhood estimate. \n",
    "\n",
    "### A Simple Example: Word Prediction\n",
    "\n",
    "Your goal is to predict the next word. You have this tiny corpus (collection of text):\n",
    "\n",
    "> \"the cat sat on the mat. the cat slept.\"\n",
    "\n",
    "Let's estimate the probability `P(mat | on the)`. <br>\n",
    "That is, given the words \"on the\", what's the chance the next word is \"mat\"?\n",
    "\n",
    "1.  **Count the sequence**: How many times do we see \"**on the**\" in the corpus?\n",
    "    *   The sequences are: `[the, cat]`, `[cat, sat]`, `[sat, on]`, `[on, the]`, <br>\n",
    "            `[the, mat]`, `[mat, the]`, `[the, cat]`, `[cat, slept]`.\n",
    "    *   `\"on the\"` appears **once**.\n",
    "\n",
    "2.  **Count the specific outcome**: How many times does \"**on the**\" is followed by \"**mat**\"?\n",
    "    *   Looking at the sequence `[on, the, mat]`, it happens **once**.\n",
    "\n",
    "3.  **Compute the Relative Frequency (The MLE)**:\n",
    "\n",
    "    $$\n",
    "    P_{MLE}(\\text{``mat''}|\\text{``on the''}) = \n",
    "    \\frac{ Count(\\text{``on the mat''}) } { Count(\\text{``on the''}) } =\n",
    "    \\frac{1}{1} = 1\n",
    "    $$\n",
    "\n",
    "According to MLE, if you see \"on the\", the next word will *always* be \"mat\" because that's all we've observed.\n",
    "\n",
    "<br>\n",
    "\n",
    "Hence, the probability $P(w_n | w_1, \\ldots, w_{n-1}, w_{n-2})$ is given by:\n",
    "\n",
    "$$\n",
    "P_{MLE}(w_n | w_{1}, w_2 \\ldots, w_{n-1}) = \n",
    "\\frac{Count(w_1, w_2, \\ldots, w_{n-1}, w_{n})}{Count(w_1, w_2, \\ldots, w_{n-1})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1c2914",
   "metadata": {},
   "source": [
    "## Markov assumption\n",
    "\n",
    "In general, we want `n` to be as large as possible. However, in practice, most events of encountering\n",
    "sequences of words of length greater than 3 hardly ever occur in our corpora!\n",
    "\n",
    "The markov assumption states that:\n",
    "\n",
    "$$\n",
    "\\text{``Only the prior local context – the last few words –\n",
    "affects the next word''}\n",
    "$$\n",
    "\n",
    "\n",
    "Suppose, we assume that for a given word, only the previous $N$ (capital `N`) are required.\n",
    "Then the probability of the $n_{th}$ word is given by:\n",
    "\n",
    "$$\n",
    "P(w_n | w_1, w_2, \\ldots, w_{n-1}) \\approx\n",
    "P(w_n | w_{(n-N+1)}, \\ldots, w_{n-2}, w_{n-1})\n",
    "$$\n",
    "\n",
    "This is known as **N grams model**.\n",
    "\n",
    "| N grams model | value of N    |\n",
    "|---            |---            |\n",
    "| unigram       | 1             |\n",
    "| biagram       | 2             |\n",
    "| trigram       | 3             |\n",
    "\n",
    "\n",
    "- Unigram: P(dog)\n",
    "- Bigram: P(dog|big)\n",
    "- Trigram: P(dog|the,big)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ecd042",
   "metadata": {},
   "source": [
    "## Problems and remedies in N grams\n",
    "\n",
    "We might have 0 probabilities for some sequences, which makes it impossible for the \n",
    "model to output unseen sequences, and generalize\n",
    "\n",
    "To solve this issue, we use `smoothing` to assign non-zero probabilities to zero-probability\n",
    "n-grams.\n",
    "\n",
    "Smoothing can be easily implemented through `Laplace's law` which basically adds 1 to all frequencies.\n",
    "\n",
    "Also, we can try lower order n-grams when higher order n-grams are not available. This idea\n",
    "is called `backoff`. For example,\n",
    "\n",
    "$$\n",
    "P(\\text{``the cat is cute''}|\\text{``the cat is''}) =\n",
    "\\frac{ Count(\\text{``the cat is cute''}) } { Count(\\text{``the cat is''}) }\n",
    "$$\n",
    "\n",
    "But what happens when we do not have any `the cat is` in our dataset i.e. Count(\"the cat is\") = 0.\n",
    "In this case, we cannot calculate the 4 grams. And we fall back to 3 grams.\n",
    "\n",
    "$$\n",
    "P(\\text{``cat is cute''}|\\text{``cat is''}) =\n",
    "\\frac{ Count(\\text{``cat is cute''}) } { Count(\\text{``cat is''}) }\n",
    "$$\n",
    "\n",
    "If even 3 grams is not available, then we can try 2 grams and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81459494",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a54a1a2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c576c83",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
